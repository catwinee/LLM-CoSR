# Introduction

*LLM-CoSR* is a LLM-driven deep learning recommendation system based on a graph contrastive learning (GCL) framework.
It is meticulously designed for API recommendation scenarios for web service developers.
The architecture comprises four synergistic components:
1. Service and user requirement encoding module;
2. LLM-driven invocation network denoising module;
3. Graph encoding and contrastive learning module;
4. Negative sampling module.

> This project is built on open source project [Lightning-Hydra-Template](https://github.com/ashleve/lightning-hydra-template).

## Motivation

Existing approaches face three gaps: 
1. Traditional QoS/matrix methods lack deep dependency modeling and cold-start adaptability due to manual feature engineering; 
2. Graph-based methods (GNNs/GCL) suffer from semantic inconsistency from unstable edge augmentation and rigid denoising assumptions; 
3. LLM-enhanced techniques primarily refine auxiliary data rather than directly aligning semantic reasoning with graph structures. We propose a unified framework combining adaptive graph diffusion (preserving functional dependencies during denoising) and LLM-powered semantic coordination, aiming to resolve data sparsity, stabilize training, and enable interpretable service-requirement alignment.


# Project Structure

The directory structure of new project looks like this:

```
│   .env.example                   <- Template of the file for storing private environment variables
│   .gitignore                     <- List of files/folders ignored by git
│   .pre-commit-config.yaml        <- Configuration of pre-commit hooks for code formatting
│   README.md
│   test.py                        <- Run testing
│   train.py                       <- Run training
│
├───configs                        <- Hydra configuration files
│   │   test.yaml                     <- Main config for testing
│   │   train.yaml                    <- Main config for training
│   ├───callbacks                  <- Lightning callbacks
│   │       wandb.yaml                <- Wandb and metrics callbacks
│   ├───datamodule                    
│   │       partial_text_bert.yaml    <- Partial text-based dataset embedded by BERT configs
│   ├───experiment                 <- Experiment configs
│   ├───hparams_search             <- Hyperparameter search configs
│   ├───logger                     <- Logger configs
│   ├───log_dir                    <- Logging directory configs
│   ├───model                      <- Model configs
│   └───trainer                    <- Trainer configs
│
├───data                        <- Project data
├───logs                        <- Logs generated by Hydra and PyTorch Lightning loggers
├───src                         <- Source code
│   │   testing_pipeline.py
│   │   training_pipeline.py
│   │
│   ├───callbacks
│   │       wandb_callbacks.py
│   ├───datamodules             <- Lightning datamodules
│   ├───models                  <- Lightning models
│   ├───utils                   <- Utility scripts
│   └───vendor                  <- Third party code that cannot be installed using PIP/Conda
└───tests                       <- Tests of any kind
    ├───helpers                    <- A couple of testing utilities
    ├───shell                      <- Shell/command based tests
    └───unit                       <- Unit tests
```

# Installation
You can install environment by anaconda, and then download the dataset.

pip install -r requirements.txt

# Dataset
The experimental dataset comprises service metadata collected from ProgrammableWeb, featuring two core components: 932 Web APIs and 4,557 mashups. These entries are interconnected through 7,139 explicit invocation records, forming dependency edges that capture service composition patterns in real-world web applications.

Download dataset here(TODO).

# Quickstart

## Basic Experiment Execution
```bash
# Launch LLM-driven contrastive learning with predefined configuration
python train.py experiment=contras-mm
```

## Customized Experiment Configuration
```bash
# Run text-to-latent experiment with deterministic seeding
python train.py experiment=t2l2 ++seed=12345
```
